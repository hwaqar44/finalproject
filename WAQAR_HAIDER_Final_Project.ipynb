{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing numpy  and pandas libs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing model_selection for spliting data and other purposes\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "%matplotlib inline\n",
    "\n",
    "# we don't want warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>spacy_similarity</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>...</th>\n",
       "      <th>cityblock_dis</th>\n",
       "      <th>jaccard_dis</th>\n",
       "      <th>canberra_dis</th>\n",
       "      <th>euclidean_dis</th>\n",
       "      <th>minkowski_dis</th>\n",
       "      <th>braycurtis_dis</th>\n",
       "      <th>skew_q1</th>\n",
       "      <th>skew_q2</th>\n",
       "      <th>kur_q1</th>\n",
       "      <th>kur_q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.313089</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>-31</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.186985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>206.461737</td>\n",
       "      <td>1.311902</td>\n",
       "      <td>1.311902</td>\n",
       "      <td>0.872352</td>\n",
       "      <td>-0.155758</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>0.188890</td>\n",
       "      <td>-0.115978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.067930</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>-66</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.787235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085398</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.117636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.664722</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>-9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11.457092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155.035952</td>\n",
       "      <td>0.849427</td>\n",
       "      <td>0.849427</td>\n",
       "      <td>0.452261</td>\n",
       "      <td>0.065101</td>\n",
       "      <td>0.079091</td>\n",
       "      <td>-0.007587</td>\n",
       "      <td>-0.470184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.275611</td>\n",
       "      <td>19</td>\n",
       "      <td>56</td>\n",
       "      <td>-37</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.435374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203.955915</td>\n",
       "      <td>1.256324</td>\n",
       "      <td>1.256324</td>\n",
       "      <td>0.830043</td>\n",
       "      <td>-0.007600</td>\n",
       "      <td>0.018465</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.015438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.587580</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.968091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>178.222529</td>\n",
       "      <td>1.086451</td>\n",
       "      <td>1.086451</td>\n",
       "      <td>0.631376</td>\n",
       "      <td>0.027209</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>-0.463481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  spacy_similarity  len_q1  len_q2  diff_len  len_char_q1  \\\n",
       "0             0          0.313089       5      36       -31            4   \n",
       "1             0         -0.067930       5      71       -66            5   \n",
       "2             0          0.664722      23      32        -9           12   \n",
       "3             0          0.275611      19      56       -37           10   \n",
       "4             0          0.587580      50      30        20           21   \n",
       "\n",
       "   len_char_q2  len_word_q1  len_word_q2  common_words  ...  cityblock_dis  \\\n",
       "0           16            1            7             0  ...      18.186985   \n",
       "1           24            1           12             0  ...      13.787235   \n",
       "2           13            4            6             1  ...      11.457092   \n",
       "3           22            4           15             0  ...      17.435374   \n",
       "4           15           10            6             0  ...      14.968091   \n",
       "\n",
       "   jaccard_dis  canberra_dis  euclidean_dis  minkowski_dis  braycurtis_dis  \\\n",
       "0          1.0    206.461737       1.311902       1.311902        0.872352   \n",
       "1          1.0    300.000000       1.000000       1.000000        1.000000   \n",
       "2          1.0    155.035952       0.849427       0.849427        0.452261   \n",
       "3          1.0    203.955915       1.256324       1.256324        0.830043   \n",
       "4          1.0    178.222529       1.086451       1.086451        0.631376   \n",
       "\n",
       "    skew_q1   skew_q2    kur_q1    kur_q2  \n",
       "0 -0.155758 -0.087513  0.188890 -0.115978  \n",
       "1  0.000000  0.085398 -3.000000  0.117636  \n",
       "2  0.065101  0.079091 -0.007587 -0.470184  \n",
       "3 -0.007600  0.018465  0.056253  0.015438  \n",
       "4  0.027209  0.037700  0.021048 -0.463481  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading training data from dir\n",
    "\n",
    "df2 = pd.read_csv(filepath_or_buffer='data/finalproject/train.csv')\n",
    "df = df2.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "What is the step by step guide to invest in share market?\n",
      "\n",
      "What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n",
      "\n",
      "How can I increase the speed of my internet connection while using a VPN?\n",
      "How can Internet speed be increased by hacking through DNS?\n",
      "\n",
      "Why am I mentally very lonely? How can I solve it?\n",
      "Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "\n",
      "Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?\n",
      "Which fish would survive in salt water?\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "\n",
      "Should I buy tiago?\n",
      "What keeps childern active and far from phone and video games?\n",
      "\n",
      "How can I be a good geologist?\n",
      "What should I do to be a great geologist?\n",
      "\n",
      "When do you use シ instead of し?\n",
      "When do you use \"&\" instead of \"and\"?\n",
      "\n",
      "Motorola (company): Can I hack my Charter Motorolla DCX3400?\n",
      "How do I hack Motorola DCX3400 for free internet?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop NA\n",
    "df = df.dropna(how=\"any\").reset_index(drop=True)\n",
    "\n",
    "a = 0\n",
    "\n",
    "for i in range(a, a+10):\n",
    "    print(df.question1[i])\n",
    "    print(df.question2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#English stopwords, a, an, the, this, that ...\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What would  a Trump presidency mean for current international master's students on an F1 visa?\"\n",
    "question2 = \"how will a Tump presidency affect the students presently in US or planning to study in US?\"\n",
    "\n",
    "# sptliting words in each sentance\n",
    "\n",
    "question1 = question1.lower().split()\n",
    "question2 = question2.lower().split()\n",
    "\n",
    "# checking what is the affect of stop_words on the sentance structure,\n",
    "# Removing the stop_words from the quesitons\n",
    "\n",
    "question1 = [w for w in question1 if w not in stop_words]\n",
    "question2 = [w for w in question2 if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training genism model with Google news data\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading important libraries, we may use in the future\n",
    "\n",
    "from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedLineDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 2.3514\n"
     ]
    }
   ],
   "source": [
    "#Checking how the WMD_Distance compare the two sentances\n",
    "# WMD distance is how much one sentance to similar to another one\n",
    "\n",
    "distance = model.wmdistance(question1, question2)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance 0.9461\n"
     ]
    }
   ],
   "source": [
    "# We can compute the normalize WMD by init_sims method, \n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "distance = model.wmdistance(question1, question2)\n",
    "\n",
    "print('distance %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 0.4169\n"
     ]
    }
   ],
   "source": [
    "question3 = \"How do we prepare for UPSC?\"\n",
    "question4 = \"How do I prepare for civil service?\"\n",
    "\n",
    "distance = model.wmdistance(question3, question4)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3247870206832886"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wmdistance([\"presently\"], [\"this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 0.2459\n"
     ]
    }
   ],
   "source": [
    "question5 = \"What is the stall speed and AOA of an f-14 with wings fully swept back?\"\n",
    "question6 = \"Why did aircraft stop using variable-sweep wings, like those on an F-14?\"\n",
    "\n",
    "distance = model.wmdistance(question5, question6)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 0.2459\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)\n",
    "distance = model.wmdistance(question5, question6)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tr out Fuzzywuzzy lib\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "fuzz.ratio(question1, question2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see the fuzz lib gives us ratios of how different questions are similar to others, while it may be\n",
    "# a bit harder for it to computer the ratios accuratly, it does it best to compute. We will use different lib like this one \n",
    "# to computer the ratio of all the questions and than use those features in our model\n",
    "\n",
    "fuzz.ratio(question4, question3),fuzz.ratio(question5, question6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for WMD distance for each question\n",
    "\n",
    "def wmd(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    \n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    \n",
    "    return model.wmdistance(q1, q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = nltk.word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    M = []\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v/np.sqrt((v**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize WMD distance\n",
    "\n",
    "def norm_wmd(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    \n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    \n",
    "    return norm_model.wmdistance(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop_words, a, an, the, and, at etc\n",
    "\n",
    "def remove_stopwords(q1):\n",
    "    q1 = str(q1).lower().split()\n",
    "    \n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    \n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words from each question for further processing\n",
    "# the models that process the sentances when looked at similar text, the question is treated as similar even if stop words are similar\n",
    "# Removing those can be benificial to us\n",
    "\n",
    "df['question1'] = df['question1'].apply(lambda x: \" \".join(remove_stopwords(x)))\n",
    "df['question2'] = df['question2'].apply(lambda x:  \" \".join(remove_stopwords(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing, stemming, root words\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# Word stemming, removing tokens, confining words to its roots, programing -> program etc...\n",
    "def root_words(q1):\n",
    "    words = word_tokenize(q1)\n",
    "    sent = [ps.stem(w) for w in words]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  changing the words to root words, this will help us recognizing the text in future from each other\n",
    "# the models will be able to compare text easily even if the form of word is changed\n",
    "\n",
    "df['question1'] = df['question1'].apply(lambda x:  \" \".join(root_words(x)))\n",
    "df['question2'] = df['question1'].apply(lambda x:  \" \".join(root_words(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing similar words from both question for comparison\n",
    "\n",
    "def remov_same(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    \n",
    "    q1 = [w for w in q1 if ps.stem(w) not in q2]\n",
    "    \n",
    "    return  \" \".join(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the same words, for comparison. we will have both questions with all the words and removed similar words for\n",
    "# comparison purposes\n",
    "\n",
    "df['question1_diff'] = df.apply(lambda x: remov_same(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "df['question2_diff'] = df.apply(lambda x: remov_same(x[\"question2\"], x[\"question1\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spaCy Similarity checking\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def spacy_sam(q1, q2):\n",
    "    q1 = nlp(q1)\n",
    "    q2 = nlp(q2)\n",
    "    \n",
    "    return q1.similarity(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Similarity between questions that have no same words in them\n",
    "\n",
    "df['spacy_similarity'] = df.apply(lambda x: spacy_sam(x['question1_diff'], x['question2_diff']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy similarity between the questions with root words similar\n",
    "\n",
    "df['spacy_sameqs_similarity'] = df.apply(lambda x: spacy_sam(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of each string in question 1 and 2\n",
    "df['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "df['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# difference between question 1 and 2 lengths\n",
    "df['diff_len'] = df['len_q1'] - df['len_q2']\n",
    "\n",
    "# number of char in of each string in question 1 and 2\n",
    "df['len_char_q1'] = df['question1'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "df['len_char_q2'] = df['question2'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "\n",
    "# length of each word in question 1 and 2\n",
    "df['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "df['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# common words\n",
    "df['common_words'] = df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "# Fuzz Ratio, the similarity or differences between each question, there are different methods in \n",
    "# fuzz class that we are using, each has its own formula of calculation\n",
    "\n",
    "df['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "df['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "df['fuzz_partial_token_sort_ratio'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "df['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzz ratios between question1 and question2\n",
    "df['fuzz_ratio_same'] = df.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "df['fuzz_partial_ratio_same'] = df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "df['fuzz_partial_token_set_ratio_same'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "df['fuzz_partial_token_sort_ratio_same'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "df['fuzz_token_set_ratio_same'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "df['fuzz_token_sort_ratio_same'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>spacy_similarity</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>...</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>fuzz_partial_token_set_ratio</th>\n",
       "      <th>fuzz_partial_token_sort_ratio</th>\n",
       "      <th>fuzz_token_set_ratio</th>\n",
       "      <th>fuzz_token_sort_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>india</td>\n",
       "      <td>step step guid invest share market ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313089</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>-31</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>stori</td>\n",
       "      <td>would happen indian govern stole kohinoor ( ko...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.067930</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>-66</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increas connect use vpn</td>\n",
       "      <td>internet speed increas hack dn ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664722</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>-9</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "      <td>100</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mental lone solv it</td>\n",
       "      <td>find remaind [ math ] 23^ { 24 } [ /math ] div...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.275611</td>\n",
       "      <td>19</td>\n",
       "      <td>56</td>\n",
       "      <td>-37</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>42</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolv quikli sugar , , methan carbon di ...</td>\n",
       "      <td>fish would surviv salt water ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587580</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2                                              india   \n",
       "1   1     3     4                                              stori   \n",
       "2   2     5     6                            increas connect use vpn   \n",
       "3   3     7     8                                mental lone solv it   \n",
       "4   4     9    10  one dissolv quikli sugar , , methan carbon di ...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0               step step guid invest share market ?             0   \n",
       "1  would happen indian govern stole kohinoor ( ko...             0   \n",
       "2                   internet speed increas hack dn ?             0   \n",
       "3  find remaind [ math ] 23^ { 24 } [ /math ] div...             0   \n",
       "4                     fish would surviv salt water ?             0   \n",
       "\n",
       "   spacy_similarity  len_q1  len_q2  diff_len  ...  len_char_q2  len_word_q1  \\\n",
       "0          0.313089       5      36       -31  ...           16            1   \n",
       "1         -0.067930       5      71       -66  ...           24            1   \n",
       "2          0.664722      23      32        -9  ...           13            4   \n",
       "3          0.275611      19      56       -37  ...           22            4   \n",
       "4          0.587580      50      30        20  ...           15           10   \n",
       "\n",
       "   len_word_q2  common_words  fuzz_ratio  fuzz_partial_ratio  \\\n",
       "0            7             0          20                  40   \n",
       "1           12             0          13                  60   \n",
       "2            6             1          40                  48   \n",
       "3           15             0          21                  32   \n",
       "4            6             0          38                  40   \n",
       "\n",
       "   fuzz_partial_token_set_ratio  fuzz_partial_token_sort_ratio  \\\n",
       "0                            40                             40   \n",
       "1                            60                             60   \n",
       "2                           100                             57   \n",
       "3                            37                             42   \n",
       "4                            39                             39   \n",
       "\n",
       "   fuzz_token_set_ratio  fuzz_token_sort_ratio  \n",
       "0                    24                     21  \n",
       "1                     9                      9  \n",
       "2                    60                     53  \n",
       "3                    29                     27  \n",
       "4                    32                     32  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMD distance between each question with same text included\n",
    "\n",
    "df['wmd'] = df.apply(lambda x: wmd(x['question1_diff'], x['question2_diff']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WMD distance between each question with root words, words can be same in each question\n",
    "\n",
    "df['wmd_same'] = df.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with google News, for normalize distance WMD \n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize WMD disntance\n",
    "\n",
    "df['norm_wmd'] = df.apply(lambda x: norm_wmd(x['question1_diff'], x['question2_diff']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized WMD distance for question including same words\n",
    "\n",
    "df['norm_wmd_same'] = df.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6981db5fc29345c9bc74c729355ffbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=404287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9f528f36fe4ca3854bb5010e777b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=404287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "question1_vector = np.zeros((df.shape[0], 300))\n",
    "\n",
    "for i, q in enumerate(tqdm_notebook(df['question1'].values)):\n",
    "    question1_vector[i, :] = sent2vec(q)\n",
    "    \n",
    "question2_vector = np.zeros((df.shape[0], 300))\n",
    "\n",
    "for i, q in enumerate(tqdm_notebook(df['question2'].values)):\n",
    "    question2_vector[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as dis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different distances between each question as features\n",
    "\n",
    "df['cosine_dis'] = [dis.cosine(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['cityblock_dis'] = [dis.cityblock(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['jaccard_dis'] = [dis.jaccard(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['canberra_dis'] = [dis.canberra(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['euclidean_dis'] = [dis.euclidean(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['minkowski_dis'] = [dis.minkowski(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['braycurtis_dis'] = [dis.braycurtis(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "df['skew_q1'] = [skew(x) for x in np.nan_to_num(question1_vector)]\n",
    "\n",
    "df['skew_q2'] = [skew(x) for x in np.nan_to_num(question2_vector)]\n",
    "\n",
    "df['kur_q1'] = [kurtosis(x) for x in np.nan_to_num(question1_vector)]\n",
    "\n",
    "df['kur_q2'] = [kurtosis(x) for x in np.nan_to_num(question2_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing nulls\n",
    "\n",
    "df[df==np.inf]=np.nan\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate',\n",
       "       'spacy_similarity', 'len_q1', 'len_q2', 'diff_len', 'len_char_q1',\n",
       "       'len_char_q2', 'len_word_q1', 'len_word_q2', 'common_words',\n",
       "       'fuzz_ratio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
       "       'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio',\n",
       "       'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cosine_dis',\n",
       "       'cityblock_dis', 'jaccard_dis', 'canberra_dis', 'euclidean_dis',\n",
       "       'minkowski_dis', 'braycurtis_dis', 'skew_q1', 'skew_q2', 'kur_q1',\n",
       "       'kur_q2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['question1', 'question2', 'id', 'qid1', 'qid2'], axis=1, inplace=True)\n",
    "df.drop(['question1', 'question2'], axis=1, inplace=True)\n",
    "\n",
    "#df = df[pd.notnull(df['cosine_dis'])]\n",
    "#df = df[pd.notnull(df['jaccard_dis'])]\n",
    "\n",
    "X = df.loc[:, df.columns != 'is_duplicate']\n",
    "y = df.loc[:, df.columns == 'is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404287 entries, 0 to 404286\n",
      "Data columns (total 38 columns):\n",
      "is_duplicate                          404287 non-null int64\n",
      "spacy_similarity                      404287 non-null float64\n",
      "len_q1                                404287 non-null int64\n",
      "len_q2                                404287 non-null int64\n",
      "diff_len                              404287 non-null int64\n",
      "len_char_q1                           404287 non-null int64\n",
      "len_char_q2                           404287 non-null int64\n",
      "len_word_q1                           404287 non-null int64\n",
      "len_word_q2                           404287 non-null int64\n",
      "common_words                          404287 non-null int64\n",
      "fuzz_ratio                            404287 non-null int64\n",
      "fuzz_partial_ratio                    404287 non-null int64\n",
      "fuzz_partial_token_set_ratio          404287 non-null int64\n",
      "fuzz_partial_token_sort_ratio         404287 non-null int64\n",
      "fuzz_token_set_ratio                  404287 non-null int64\n",
      "fuzz_token_sort_ratio                 404287 non-null int64\n",
      "wmd                                   404287 non-null float64\n",
      "norm_wmd                              404287 non-null float64\n",
      "cosine_dis                            404287 non-null float64\n",
      "cityblock_dis                         404287 non-null float64\n",
      "jaccard_dis                           404287 non-null float64\n",
      "canberra_dis                          404287 non-null float64\n",
      "euclidean_dis                         404287 non-null float64\n",
      "minkowski_dis                         404287 non-null float64\n",
      "braycurtis_dis                        404287 non-null float64\n",
      "skew_q1                               404287 non-null float64\n",
      "skew_q2                               404287 non-null float64\n",
      "kur_q1                                404287 non-null float64\n",
      "kur_q2                                404287 non-null float64\n",
      "spacy_sameqs_similarity               404287 non-null float64\n",
      "fuzz_ratio_same                       404287 non-null int64\n",
      "fuzz_partial_ratio_same               404287 non-null int64\n",
      "fuzz_partial_token_set_ratio_same     404287 non-null int64\n",
      "fuzz_partial_token_sort_ratio_same    404287 non-null int64\n",
      "fuzz_token_set_ratio_same             404287 non-null int64\n",
      "fuzz_token_sort_ratio_same            404287 non-null int64\n",
      "wmd_same                              404287 non-null float64\n",
      "norm_wmd_same                         404287 non-null float64\n",
      "dtypes: float64(17), int64(21)\n",
      "memory usage: 120.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323429, 37), (80858, 37), (323429, 1), (80858, 1))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "#splitting the dataset to training and testing sets\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = model_selection.train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "# checking the shape of our data sets\n",
    "\n",
    "X_train.shape, X_holdout.shape, y_train.shape, y_holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>spacy_similarity</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>...</th>\n",
       "      <th>cityblock_dis</th>\n",
       "      <th>jaccard_dis</th>\n",
       "      <th>canberra_dis</th>\n",
       "      <th>euclidean_dis</th>\n",
       "      <th>minkowski_dis</th>\n",
       "      <th>braycurtis_dis</th>\n",
       "      <th>skew_q1</th>\n",
       "      <th>skew_q2</th>\n",
       "      <th>kur_q1</th>\n",
       "      <th>kur_q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.313089</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>-31</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.186985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>206.461737</td>\n",
       "      <td>1.311902</td>\n",
       "      <td>1.311902</td>\n",
       "      <td>0.872352</td>\n",
       "      <td>-0.155758</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>0.188890</td>\n",
       "      <td>-0.115978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.067930</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>-66</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.787235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085398</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.117636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.664722</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>-9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11.457092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155.035952</td>\n",
       "      <td>0.849427</td>\n",
       "      <td>0.849427</td>\n",
       "      <td>0.452261</td>\n",
       "      <td>0.065101</td>\n",
       "      <td>0.079091</td>\n",
       "      <td>-0.007587</td>\n",
       "      <td>-0.470184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.275611</td>\n",
       "      <td>19</td>\n",
       "      <td>56</td>\n",
       "      <td>-37</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.435374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203.955915</td>\n",
       "      <td>1.256324</td>\n",
       "      <td>1.256324</td>\n",
       "      <td>0.830043</td>\n",
       "      <td>-0.007600</td>\n",
       "      <td>0.018465</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.015438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.587580</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.968091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>178.222529</td>\n",
       "      <td>1.086451</td>\n",
       "      <td>1.086451</td>\n",
       "      <td>0.631376</td>\n",
       "      <td>0.027209</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>-0.463481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>32</td>\n",
       "      <td>61</td>\n",
       "      <td>-29</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.452011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198.441203</td>\n",
       "      <td>1.251492</td>\n",
       "      <td>1.251492</td>\n",
       "      <td>0.785737</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>0.013003</td>\n",
       "      <td>-0.173295</td>\n",
       "      <td>-0.556737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.253870</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>-33</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.937801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199.486199</td>\n",
       "      <td>1.243934</td>\n",
       "      <td>1.243934</td>\n",
       "      <td>0.781724</td>\n",
       "      <td>-0.090366</td>\n",
       "      <td>0.042329</td>\n",
       "      <td>0.296892</td>\n",
       "      <td>-0.347765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.628369</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>-13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.644691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.087853</td>\n",
       "      <td>0.997961</td>\n",
       "      <td>0.997961</td>\n",
       "      <td>0.588381</td>\n",
       "      <td>0.039933</td>\n",
       "      <td>0.456804</td>\n",
       "      <td>-0.165647</td>\n",
       "      <td>1.210527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>-28</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.227400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205292</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.409441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.356729</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>-6</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.214457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>216.637528</td>\n",
       "      <td>1.398589</td>\n",
       "      <td>1.398589</td>\n",
       "      <td>0.965334</td>\n",
       "      <td>-0.023789</td>\n",
       "      <td>0.169342</td>\n",
       "      <td>0.041027</td>\n",
       "      <td>-0.182079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  spacy_similarity  len_q1  len_q2  diff_len  len_char_q1  \\\n",
       "0             0          0.313089       5      36       -31            4   \n",
       "1             0         -0.067930       5      71       -66            5   \n",
       "2             0          0.664722      23      32        -9           12   \n",
       "3             0          0.275611      19      56       -37           10   \n",
       "4             0          0.587580      50      30        20           21   \n",
       "5             1          0.571653      32      61       -29           15   \n",
       "6             0          0.253870       9      42       -33            8   \n",
       "7             1          0.628369       4      17       -13            3   \n",
       "8             0          0.000000       3      31       -28            2   \n",
       "9             0          0.356729      31      37        -6           15   \n",
       "\n",
       "   len_char_q2  len_word_q1  len_word_q2  common_words  ...  cityblock_dis  \\\n",
       "0           16            1            7             0  ...      18.186985   \n",
       "1           24            1           12             0  ...      13.787235   \n",
       "2           13            4            6             1  ...      11.457092   \n",
       "3           22            4           15             0  ...      17.435374   \n",
       "4           15           10            6             0  ...      14.968091   \n",
       "5           20            7           14             0  ...      17.452011   \n",
       "6           18            2            8             0  ...      16.937801   \n",
       "7           10            1            3             0  ...      13.644691   \n",
       "8           12            2            9             0  ...      14.227400   \n",
       "9           19            6            6             0  ...      19.214457   \n",
       "\n",
       "   jaccard_dis  canberra_dis  euclidean_dis  minkowski_dis  braycurtis_dis  \\\n",
       "0          1.0    206.461737       1.311902       1.311902        0.872352   \n",
       "1          1.0    300.000000       1.000000       1.000000        1.000000   \n",
       "2          1.0    155.035952       0.849427       0.849427        0.452261   \n",
       "3          1.0    203.955915       1.256324       1.256324        0.830043   \n",
       "4          1.0    178.222529       1.086451       1.086451        0.631376   \n",
       "5          1.0    198.441203       1.251492       1.251492        0.785737   \n",
       "6          1.0    199.486199       1.243934       1.243934        0.781724   \n",
       "7          1.0    177.087853       0.997961       0.997961        0.588381   \n",
       "8          1.0    300.000000       1.000000       1.000000        1.000000   \n",
       "9          1.0    216.637528       1.398589       1.398589        0.965334   \n",
       "\n",
       "    skew_q1   skew_q2    kur_q1    kur_q2  \n",
       "0 -0.155758 -0.087513  0.188890 -0.115978  \n",
       "1  0.000000  0.085398 -3.000000  0.117636  \n",
       "2  0.065101  0.079091 -0.007587 -0.470184  \n",
       "3 -0.007600  0.018465  0.056253  0.015438  \n",
       "4  0.027209  0.037700  0.021048 -0.463481  \n",
       "5  0.104864  0.013003 -0.173295 -0.556737  \n",
       "6 -0.090366  0.042329  0.296892 -0.347765  \n",
       "7  0.039933  0.456804 -0.165647  1.210527  \n",
       "8  0.000000  0.205292 -3.000000 -0.409441  \n",
       "9 -0.023789  0.169342  0.041027 -0.182079  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "xb = xgb.XGBClassifier()\n",
    "xb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6918053872220435"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkng score on test set\n",
    "\n",
    "xb.score(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(X_train.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with NLP Classifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, ), activation='relu')\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6862648099136758"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP Scoring\n",
    "\n",
    "mlp.score(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713497736773109"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking with Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "clf.score(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The XGBoost score was higher so we will tune that model only, we don't need other models for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.02], #so called `eta` value\n",
    "              'max_depth': np.arange(4, 10),\n",
    "              'min_child_weight': np.arange(7, 10),\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.5],\n",
    "              'n_estimators': [500]}\n",
    "xgb_tunned = GridSearchCV(xgb.XGBClassifier(), parameters, n_jobs=-1, verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed: 87.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 30min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'nthread': [4], 'objective': ['binary:logistic'], 'learning_rate': [0.02], 'max_depth': array([4, 5, 6, 7, 8, 9]), 'min_child_weight': array([7, 8, 9]), 'silent': [1], 'subsample': [0.8], 'colsample_bytree': [0.5], 'n_estimators': [500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_tunned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7125825521284227"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_tunned.score(X_holdout, y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking that feature engineering on original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_csv(filepath_or_buffer='data/finalproject/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NA\n",
    "dftest = dftest.dropna(how=\"any\").reset_index(drop=True)\n",
    "\n",
    "# Removing stop words from each question for further processing\n",
    "# the models that process the sentances when looked at similar text, the question is treated as similar even if stop words are similar\n",
    "# Removing those can be benificial to us\n",
    "\n",
    "dftest['question1'] = dftest['question1'].apply(lambda x: \" \".join(remove_stopwords(x)))\n",
    "dftest['question2'] = dftest['question2'].apply(lambda x:  \" \".join(remove_stopwords(x)))\n",
    "\n",
    "#  changing the words to root words, this will help us recognizing the text in future from each other\n",
    "# the models will be able to compare text easily even if the form of word is changed\n",
    "\n",
    "dftest['question1'] = dftest['question1'].apply(lambda x:  \" \".join(root_words(x)))\n",
    "dftest['question2'] = dftest['question1'].apply(lambda x:  \" \".join(root_words(x)))\n",
    "\n",
    "# removing the same words, for comparison. we will have both questions with all the words and removed similar words for\n",
    "# comparison purposes\n",
    "\n",
    "dftest['question1_diff'] = dftest.apply(lambda x: remov_same(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "dftest['question2_diff'] = dftest.apply(lambda x: remov_same(x[\"question2\"], x[\"question1\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Similarity between questions that have no same words in them\n",
    "\n",
    "dftest['spacy_similarity'] = dftest.apply(lambda x: spacy_sam(x['question1_diff'], x['question2_diff']), axis=1)\n",
    "\n",
    "# Spacy similarity between the questions with root words similar\n",
    "\n",
    "dftest['spacy_sameqs_similarity'] = dftest.apply(lambda x: spacy_sam(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of each string in question 1 and 2\n",
    "dftest['len_q1'] = dftest['question1'].apply(lambda x: len(str(x)))\n",
    "dftest['len_q2'] = dftest['question2'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# difference between question 1 and 2 lengths\n",
    "dftest['diff_len'] = dftest['len_q1'] - dftest['len_q2']\n",
    "\n",
    "# number of char in of each string in question 1 and 2\n",
    "dftest['len_char_q1'] = dftest['question1'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "dftest['len_char_q2'] = dftest['question2'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "\n",
    "# length of each word in question 1 and 2\n",
    "dftest['len_word_q1'] = dftest['question1'].apply(lambda x: len(str(x).split()))\n",
    "dftest['len_word_q2'] = dftest['question2'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# common words\n",
    "dftest['common_words'] = dftest.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "# Fuzz Ratio, the similarity or differences between each question, there are different methods in \n",
    "# fuzz class that we are using, each has its own formula of calculation\n",
    "\n",
    "dftest['fuzz_ratio'] = dftest.apply(lambda x: fuzz.ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "dftest['fuzz_partial_ratio'] = dftest.apply(lambda x: fuzz.partial_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "dftest['fuzz_partial_token_set_ratio'] = dftest.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "dftest['fuzz_partial_token_sort_ratio'] = dftest.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "dftest['fuzz_token_set_ratio'] = dftest.apply(lambda x: fuzz.token_set_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n",
    "\n",
    "dftest['fuzz_token_sort_ratio'] = dftest.apply(lambda x: fuzz.token_sort_ratio(str(x['question1_diff']), str(x['question2_diff'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzz ratios between question1 and question2\n",
    "dftest['fuzz_ratio_same'] = dftest.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "dftest['fuzz_partial_ratio_same'] = dftest.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "dftest['fuzz_partial_token_set_ratio_same'] = dftest.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "dftest['fuzz_partial_token_sort_ratio_same'] = dftest.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "dftest['fuzz_token_set_ratio_same'] = dftest.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "dftest['fuzz_token_sort_ratio_same'] = dftest.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMD distance between each question with same text included\n",
    "\n",
    "dftest['wmd'] = dftest.apply(lambda x: wmd(x['question1_diff'], x['question2_diff']), axis=1)\n",
    "\n",
    "#WMD distance between each question with root words, words can be same in each question\n",
    "\n",
    "dftest['wmd_same'] = dftest.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "# normalize WMD disntance\n",
    "\n",
    "dftest['norm_wmd'] = dftest.apply(lambda x: norm_wmd(x['question1_diff'], x['question2_diff']), axis=1)\n",
    "\n",
    "# Normalized WMD distance for question including same words\n",
    "\n",
    "dftest['norm_wmd_same'] = dftest.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_vector = np.zeros((dftest.shape[0], 300))\n",
    "\n",
    "for i, q in enumerate(tqdm_notebook(dftest['question1'].values)):\n",
    "    question1_vector[i, :] = sent2vec(q)\n",
    "    \n",
    "question2_vector = np.zeros((dftest.shape[0], 300))\n",
    "\n",
    "for i, q in enumerate(tqdm_notebook(dftest['question2'].values)):\n",
    "    question2_vector[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different distances between each question as features\n",
    "\n",
    "dftest['cosine_dis'] = [dis.cosine(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['cityblock_dis'] = [dis.cityblock(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['jaccard_dis'] = [dis.jaccard(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['canberra_dis'] = [dis.canberra(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['euclidean_dis'] = [dis.euclidean(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['minkowski_dis'] = [dis.minkowski(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['braycurtis_dis'] = [dis.braycurtis(x,y) for (x, y) in zip(np.nan_to_num(question1_vector), np.nan_to_num(question2_vector))]\n",
    "\n",
    "dftest['skew_q1'] = [skew(x) for x in np.nan_to_num(question1_vector)]\n",
    "\n",
    "dftest['skew_q2'] = [skew(x) for x in np.nan_to_num(question2_vector)]\n",
    "\n",
    "dftest['kur_q1'] = [kurtosis(x) for x in np.nan_to_num(question1_vector)]\n",
    "\n",
    "dftest['kur_q2'] = [kurtosis(x) for x in np.nan_to_num(question2_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
